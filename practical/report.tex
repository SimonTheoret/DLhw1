\documentclass[12pt]{article}
\usepackage[canadien]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{url}
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{tikz}
\usepackage{float}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\graphicspath{{ ./figures/ }}

% if your paper is accepted, change the options for the package
% aistats2e as follows:
%
%\usepackage[accepted]{aistats2e}
%
% this option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.
\setlength{\parindent}{0cm}
\addtolength{\oddsidemargin}{-2cm}
\addtolength{\evensidemargin}{-2cm}
\setlength{\textwidth}{17.78cm}
\addtolength{\topmargin}{-2.25cm}
\setlength{\textheight}{23.24cm}
\addtolength{\parskip}{5mm}
\pagestyle{fancy}

%************
%* commands *
%************

\input{math_commands.tex}

\newif\ifexercise
\exercisetrue
%\exercisefalse

\newif\ifsolution
\solutiontrue
%\solutionfalse

\usepackage{booktabs}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{exercise}{question}%[chapter]
\newtheorem{answer}{answer} % asterisk to remove ordering

% \newcommand{\exercise}[1]{
% \ifexercise#1\fi
% }

% \definecolor{answer}{rgb}{0.00, 0.12, 0.60}
% \newcommand{\answer}[1]{
% \ifsolution\color{answer}\begin{answer}#1\end{answer}\fi
% }

\usepackage{enumitem}
\newcommand{\staritem}{
\addtocounter{enumi}{1}
\item[$\phantom{x}^{*}$\theenumi]}
\setlist[enumerate,1]{leftmargin=*, label=\arabic*.}
\renewcommand\thesubsubsection{\Alph{subsubsection}}


\usepackage[normalem]{ulem}

\begin{document}


\fancyhead{}
\fancyfoot{}

\fancyhead[l]{
  \begin{tabular}[b]{l}
    ift6135-a2023  \\
    prof: aishwarya agrawal \\
  \end{tabular}
}
\fancyhead[r]{
  \begin{tabular}[b]{r}
    assignment 1 - programming  \\
    multilayer perceptrons and convolutional neural networks \\
  \end{tabular}
}
\fancyfoot[c]{- do not distribute -}

\vspace{1cm}

\shorthandoff{:}
{\textbf{due date: october 21th, 2023 at 11:00 pm}}\\

\vspace{-0.5cm}
\uline{the use of ai tools like chat-gpt to find answers or parts of answers for any question in this assignment is not allowed. however, you can use these tools to improve the quality of your writing, like fixing grammar or making it more understandable. if you do use these tools, you must clearly explain how you used them and which questions or parts of questions you applied them to. failing to do so or using these tools to find answers or parts of answers may result in your work being completely rejected, which means you'll receive a score of 0 for the entire practical section.}


\renewcommand{\labelitemi}{\textbullet}

\begin{itemize}
\item \emph{this assignment is intensive, start well ahead of time!}
\item \emph{for all questions, show your work.}
\item \emph{the practical part has been provided to you as a jupyter notebook on google colab. you must fill in your answers in the notebook and export the notebook as a python file named \texttt{solution.py} (file$\xrightarrow{}$download$\xrightarrow{}$download .py) and submit it via gradescope for autograding.}
\item \emph{you must also submit your report (pdf) on gradescope. your report must contain answers to questions 1.1 d), 1.2 j) k) l), and 3.5 to 3.9. you do not need to submit code for these questions, just the report will suffice.}
\item \emph{tas for this assignment are \textbf{shuo zhang and thomas jiralerspong}.}
\end{itemize}
\vspace{0.2cm}

Student name: Simon Théorêt
\textbf{link to notebook:} \href{https://colab.research.google.com/drive/1473s94Jv_ti1Hddf0_Hh1R136zOeYhTv?usp=sharing}{click here.}
\vspace{0.2cm}
\section*{Homework 1: Report}
\subsection{Implementing Perceptron and MLP with NumPy}

\subsubsection*{J)} These are the training graphics generated by the notebook:

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{./figures/mlpbatchsize16.png}
    \caption{MLP training with batch size 16}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{./figures/mlpbatchsize32.png}
    \caption{MLP training with batch size 32}
  \end{subfigure}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{./figures/mlpbatchsize64.png}
    \caption{MLP training with batch size 64}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{./figures/mlpbatchsize128.png}
    \caption{MLP training with batch size 128}
  \end{subfigure}
\end{figure}
As we can see from these figures, a small batch size makes the accuracy and loss
more noisy. The bigger the batch, the lesser the noise. We can explain this
phenomenon by the reduced variance of bigger samples during the stochastic gradient descent.

\subsubsection*{K)} These are the training graphics generated by the notebook:

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{./figures/mlplr01.png}
    \caption{MLP training with $lr$ = 0.1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{./figures/mlplr001.png}
    \caption{MLP training with $lr$ = 0.01}
  \end{subfigure}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{./figures/mlplr0001.png}
    \caption{MLP training with $lr$ = 0.001}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{./figures/mlplr00001.png}
    \caption{MLP training with $lr$ = 0.0001}
  \end{subfigure}
\end{figure}
As we can see from these figures, a big learning rate such as 0.1 doesn't allow
the model to be learn properly: it cannot converge as the steps are too
big. On the opposite side, we see that a small learning rate such as 0.0001
doesn't allow for fast convergence: the model is still learning altough it did a
full epoch of training.

\subsubsection*{L)} These are the training graphics generated by the notebook:
\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./figures/mlpactrelu}
         \caption{MLP training with ReLU activation function}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./figures/mlpactsig}
         \caption{MLP training with sigmoid activation function}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./figures/mlpacttanh}
         \caption{MLP training with tanh activation function}
     \end{subfigure}
\end{figure}
As the figures show us, the ReLU function is the best choice for this MLP and
dataset. It learns faster than the tanh function and the sigmoid function. On
the opposite, the sigmoid activation function does not allow the model to be
properly trained: there is no apparent convergence. Finally, the tanh activation
function converges slower than the ReLU activation function.

\subsection{Implementation and experimentation}
\subsubsection*{5}
I use this architecture for three main reasons. By using a deeper network, I
hoped the new model would be able to find more abstract features and therefore
would perform better on the test dataset. Typically, deeper network perform
better than shallower, \textit{ceteris paribus}.
\\
The second reason was to diminish the number of parameters in the network by
using a much smaller first fully connected layer than previously, and with one
layer completed removed from the new model.  To achieve this reduction in size,
I also used a 1 $\times$ 1 convolution layer to reduce the number of filters down to
246 right before the fully connected layers. The new model has 80M parameters.
\\
Finally, I removed all the bias present in the convolution layers. These
parameters we not useful in the vgg16, the following batch normalization would
erase the effect of the bias in the convolutions layers.
\subsubsection*{6}
% CHANGE THE FIGURE
The models were trained for a total of 15 epochs.
\begin{figure}
  \centering
  \includegraphics[scale = 1]{./figures/bothmodelssummary.png}
  \caption{VGG16 and the new model architecture performances}
\end{figure}
\subsubsection*{7}
Both models have similar performance as the number of epochs goes up. One
explication is the similarity of the models. Although the new model has about 80
millions parameters, the main differences are from the depth of the models, with
the new model being deeper than the VGG16 and the nature of the last layers.
The last layers (i.e fully connected layers) of VGG16 and the new model (conv
$1\time 1$ and two fully connected layers) have the same functions
\subsubsection*{8}
\subsubsection*{9}
\subsubsection*{10}
\end{document}
