\documentclass[12pt]{article}
\usepackage[canadien]{babel} 
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{url}
\usepackage[dvipsnames]{xcolor}
\usepackage{todonotes}

% If your paper is accepted, change the options for the package
% aistats2e as follows:
%
%\usepackage[accepted]{aistats2e}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.
\setlength{\parindent}{0cm}
\addtolength{\oddsidemargin}{-2cm}
\addtolength{\evensidemargin}{-2cm}
\setlength{\textwidth}{17.78cm}
\addtolength{\topmargin}{-2.25cm}
\setlength{\textheight}{23.24cm}
\addtolength{\parskip}{5mm}
\pagestyle{fancy}

%************
%* COMMANDS *
%************

\input{math_commands.tex}

\newif\ifexercise
\exercisetrue
%\exercisefalse

\newif\ifsolution
\solutiontrue
%\solutionfalse

\usepackage{booktabs}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{exercise}{Question}%[chapter]
\newtheorem{answer}{Answer} % asterisk to remove ordering

\newcommand{\Exercise}[1]{
\ifexercise#1\fi
}


\definecolor{answer}{rgb}{0.00, 0.12, 0.60}
\newcommand{\Answer}[1]{
\ifsolution\color{answer}\begin{answer}#1\end{answer}\fi
}

\newif\ifexercise
\exercisetrue
%\exercisefalse

\newif\ifsolution
\solutiontrue
%\solutionfalse

\newcommand{\pder}[2]{\frac{\partial #1}{\partial #2}}
\usepackage{enumitem}
\newcommand{\staritem}{
\addtocounter{enumi}{1}
\item[$\phantom{x}^{*}$\theenumi]}
\setlist[enumerate,1]{leftmargin=*, label=\arabic*.}

\begin{document}


\fancyhead{}
\fancyfoot{}

\fancyhead[L]{
  \begin{tabular}[b]{l}
    IFT6135-A2023  \\
    Prof: Aishwarya Agrawal \\
  \end{tabular}
}
\fancyhead[R]{
  \begin{tabular}[b]{r}
    Assignment 1  \\
    Multilayer Perceptrons and Convolutional Neural Networks \\
  \end{tabular}
}
\fancyfoot[C]{- Do not distribute -}

\vspace{1cm}

\shorthandoff{:}
{\textbf{Due Date: October 19th, 2023 at 11:00 pm}}\\


\vspace{-0.5cm}
\underline{Instructions}%
\renewcommand{\labelitemi}{\textbullet}

\begin{itemize}
\item \emph{For all questions, show your work!}
\item \emph{Use LaTeX and the template we provide when writing your answers.
You may reuse most of the notation shorthands, equations and/or tables.
See the assignment policy on the course website for more details.}
\item \emph{The use of AI tools like Chat-GPT to find answers or parts of answers for any question in this assignment is not allowed. However, you can use these tools to improve the quality of your writing, like fixing grammar or making it more understandable. If you do use these tools, you must clearly explain how you used them and which questions or parts of questions you applied them to. Failing to do so or using these tools to find answers or parts of answers may result in your work being completely rejected, which means you'll receive a score of 0 for the entire theory or practical section.}
\item \emph{Submit your answers electronically via Gradescope.}
\item \emph{TAs for this assignment are \textbf{Saba Ahmadi, Sahar Dastani, and Sophie Xhonneux.}}
\end{itemize}

\vspace{0.2cm}

% Q1
\begin{exercise}[5-2-2-1-3-2] (\textbf{Activation Functions and Backpropagation})
\Exercise{
\label{ex:activation}
\emph{Please note that during marking we will not read more than the question says you should write, e.g.\ if the question asks for one sentence, only the first sentence will be read.}
\begin{enumerate}
    \item Given the following 3 layer neural network $\vo = f(\vx)$
    \begin{equation}
    \begin{aligned}
        \vh &= \mW_1 \vx + \vb_1 \\
        \va &= \sigmoid( \mW_2 \vh + \vb_2 ) \\
        \vo &= \mW_3 \va +\vb_3,
    \end{aligned}
    \end{equation}
    where $\vx\in\mathrm{R}^n$, $\vh\in\mathrm{R}^m$, $\va\in\mathrm{R}^r$, $\vo\in\mathrm{R}^s$, $\sigmoid(x) = \frac{1}{1+e^{-x}}$, and a loss function $\mathrm{L}(\vo,\vy)$ apply the backpropagation algorithm computing the gradients with respect to the weights $\mW_1,\mW_2,\mW_3, \vb_1,\vb_2,\vb_3$ to show which are the necessary intermediate vectors that we need to store during the forward pass such as to not need recomputation. Use the notation provided above.
    \item Assuming each floating point number takes two bytes to store, give the amount of memory in bytes, needed to store during the forward pass for the backward pass to not need recomputation.
    \item Compute the left and right derivative of the \texttt{ReLU} function at $x=0$. Show your work.
    \item Give \textbf{one} sentence to explain why we cannot use the \texttt{ReLU} function if we want to differentiate our neural network twice.
    \item The swish activation function is defined as $ g(x) = x  \sigmoid(\beta  x)$. Simplify the function in the limit as $\beta$ approaches 0 and infinity. Show your work.
    \item Give 1 reason (one sentence) as to why we might want to use the swish function over the \texttt{ReLU}. Give 1 reason (one sentence) as to why we might want to use the \texttt{ReLU} over the swish (hint: monotonicity).
\end{enumerate}
}

\Answer{
% Write your answer here
% TODO: add z and \sigma in calculations...
\subsubsection*{1}
Let $z=W_{2}h+b_{2}$.
  \begin{align*}
    \pder{L}{W_1} &= \pder{L}{o} \pder{o}{a} \pder{a}{\sigma} \pder{\sigma}{z}\pder{z}{h}\pder{h}{W_{1}}\\
    &=x\pder{L}{o} W_{3} (\sigma(z)\odot(\bold{1}-\sigma(z)))W_{2}\\
    &= x\pder{L}{o} W_{3} (a \odot (\bold{1}-a)) W_{2}
  \end{align*}
  To calculate the gradient with respect to $W_{1}$, we need to store the
  vectors $x,\, a $ and the matrix and $W_{3}$
  \begin{align*}
    \pder{L}{W_2} &= \pder{L}{o}\pder{o}{a} \pder{a}{\sigma} \pder{\sigma}{W_{2}} \\
    &= h\pder{L}{o}  W_{3} \sigma(z)\odot(\bold{1}-\sigma(W_{2}h+b_{2}))\\
    &= h\pder{L}{o} W_{3} a\odot(\bold{1}-a)
  \end{align*}
  To calculate the gradient with respect to $W_{2}$, we need to store the
  vectors $ h,\, a $ and the matrix and $W_{3}$
  \begin{align*}
    \pder{L}{W_3} &= \pder{L}{o}\pder{o}{W_{3}} \\
    &= a\pder{L}{o}
  \end{align*}
  To calculate the gradient with respect to $W_{3}$, we need to store the
  vector $a$.
  \begin{align*}
    \pder{L}{b_1} &= \pder{L}{o} \pder{o}{a} \pder{a}{\sigma} \pder{\sigma}{z}\pder{z}{h}\pder{h}{b_{1}}\\
    &=x\pder{L}{o} W_{3} (\sigma(z)\odot(\bold{1}-\sigma(z)))\\
    &= x\pder{L}{o} W_{3} (a \odot (\bold{1}-a))\\
  \end{align*}
  To calculate the gradient with respect to $b_{1}$, we need to store the
  vectors $ a $ and the matrix $W_{2}$.
  \begin{align*}
    \pder{L}{b_2} &= \pder{L}{o}\pder{o}{a} \pder{a}{\sigma} \pder{\sigma}{b_{2}} \\
    &= h\pder{L}{o}  W_{3} \sigma(z)\odot(\bold{1}-\sigma(z))\\
    &= h\pder{L}{o} W_{3} a \odot (\bold{1}-a)
  \end{align*}
  To calculate the gradient with respect to $b_{2}$, we need to store the
  vectors $ a $ and the matrix $W_{3}$.
  \begin{align*}
    \pder{L}{b_{3}} &= \pder{o}{b_{3}} \pder{L}{o}\\
    &= \pder{L}{o}
  \end{align*}
  To calculate the gradient with respect to $b_{3}$, we don need to store any
  vector. We only need to compute the derivative of $L$ with respect to $o$.
\subsubsection*{2}
  To compute the gradient with respect to $W_{1}, W_{2}, W_{3}, b_{1}, b_{2}$ and
  $b_{3}$, we need to store the following vectors and matrices:
  \begin{equation*}
    x \in \R^{n}, \, a \in \R^{r}, \, h \in \R^{m}, \,W_{3} \in \R^{s \times r}, \, W_{2} \in \R^{r \times m}.
  \end{equation*}Therefore, we need a total amount of $2(n+r+m+sr+rm)$ bytes to
  store all of these numbers.
\subsubsection*{3}
Firstly, we need to calculate the right limit:
\begin{equation*}
  \lim_{x\to0} \frac{ Relu(x)-0 }{x-0} = \lim_{x\to0} \frac{ x-0 }{x-0} = 1 \quad x >Â 0
\end{equation*}This limit is obvously 1, as the expression can be reduced to 1
because $Relu(x) = x$ when $x\geq0$.
The other left side derivative is

\begin{align*}
  \lim_{x\to0} \frac{ Relu(x)-0 }{x-0} &= \lim_{x\to0} \frac{0}{x} \\
  &= 0 \quad \text{for $x$ < 0}
\end{align*}We can conclude that the left side derivative is 0 around 0.

}
\subsubsection*{4}It would make our gradients vanish (i.e. equals 0) as the second derivative of
the $Relu$ function is always $0$.
\subsubsection{5}
The swish function is defined as $g(x) = \frac{x}{1+e^{-\beta x}}$. Let's first
calculate the limit when $\beta \to 0$:
\begin{align*}
  \lim_{\beta \to 0} \frac{x}{1+e^{-\beta x}} &= \frac{x}{1+\lim_{\beta \to 0} e^{-\beta x}}\\
  &=  \frac{x}{1+ e^{\lim_{\beta \to 0}-\beta x}} \quad \text{$e^{x}$ is continuous}\\
  &=  \frac{x}{1+ e^{0}}\\
  &=  \frac{x}{2}
\end{align*}
We can calculate the limit when $\beta \to \infty$:
\begin{align*}
  \lim_{\beta \to \infty} \frac{x}{1+e^{-\beta x}} &= \frac{x}{1+\lim_{\beta \to \infty} e^{-\beta x}}\\
  &= \frac{x}{1+\lim_{\beta \to \infty} \frac{1}{e^{\beta x}}}\\
  &=  \frac{x}{1+ 0}\\
  &=  x
\end{align*}
\subsubsection*{6}
%TODO: Find a real answer
We might prefer the swish function because it is a smooth function. We might
prefer the Relu function because it is convex.
\end{exercise}

% Q2
\begin{exercise}[4-4-5-3] (\textbf{Convolution Basics})
\Exercise{
\label{ex:cnn}
\begin{enumerate}
    \item \textbf{Short answers:}
    \begin{enumerate}
        \item When you apply a filter to feature maps, under what conditions does the size of the feature map decrease?
        \item What impact does a larger stride have?
        \item Explain what you understand from the term "sparse connections" in terms of connections between two layers in a neural network.
        \item As we are aware, each \textit{Convolutional} layer contains learnable parameters, including weights and biases. If you decide not to include biases in the \textit{Convolution} layer of the following network, what will be the impact on its performance (compared to the case when the \textit{Convolution} layer includes biases)?
        $$Input \rightarrow Convolution \rightarrow Batch Norm \rightarrow Activation \rightarrow Output$$
    \end{enumerate}
    \item \textbf{True/False:}
    \begin{enumerate}
        \item In a convolutional layer, the number of weights is contingent upon the depth of the input data volume.
        \item In a convolutional layer, the number of biases equals the number of filters.
        \item The total number of parameters is influenced by the stride and padding.
        \item Maxpooling operates only on the height and width dimensions of an image.
    \end{enumerate}
    \item \textbf{Convolutional Architecture:} Let's analyze the convolutional neural network (CNN) defined by the layers in the left column. For each layer, determine the output volume's shape and the number of parameters. It's worth noting that in our notation, "Conv4-N-Pvalid-S2" represents a convolutional layer with N 4x4 filters, valid padding, and a stride of 2.

    \begin{center}
    \begin{tabular}{| c | c | c |} 
     \hline
     Layers & Output volume dimensions & Number of parameters \\ [0.5ex] 
     \hline
     Input & $64 \times 64 \times 1$ &  \\ 
     \hline
     Conv4-5-Pvalid-S2 &  &  \\
     \hline
     Pool3 &  &  \\
     \hline
     Conv3-5-Pvalid-S1 &  &  \\
     \hline
     Pool2 &  &  \\ [1ex] 
     \hline
     FC5 &  &  \\ [1ex] 
     \hline
    \end{tabular}
    \end{center}
% latexmk -pvc <file>.tex
    \item \textbf{Receptive Fields:} Receptive Field is defined as the size of the region in the input that produces a feature. For example, when using a Conv-3x3 layer, each feature is generated by a 3x3 grid in the input. Having two Conv-3x3 layers in succession leads to a 5x5 receptive field. An example of this is outlined in Figure \ref{fig:Obj-1}. Note that there is a receptive field associated with each feature (Figure \ref{fig:Obj-1} shows the receptive field associated with the yellow-colored feature in Layer 3), where a feature is a cell in the output activation map. 

    % \begin{figure}
    %   \centering
    %   \includegraphics[width=0.25\columnwidth,natwidth=610,natheight=642]{Figures/receptive.png}
    %   \caption{Illustration of Receptive Field.}
    %   \label{fig:Obj-1}
    % \end{figure}

    Consider a network architecture consisting solely of 5 Convolution layers, each with kernel size 5, stride 1, zero-padding of two pixels on all four sides, 8 intermediate channels, and one output channel. What is the receptive field of the final features obtained through this architecture, that is, what is the size of the input that corresponds to a particular pixel position in the output?
\end{enumerate}
}

\Answer{
% Write your answer here
\subsubsection*{1}
\begin{enumerate}[label=(\alph*)]
 \item For a square filter and square input without any dilatation, the feature maps
        size doesn't decrease as long as $(i-1)s -i -2p + k > 0$.
 \item It reduces the size of the feature map and the amount of computation at
        inference time.
 \item Two layers are sparsely connected if the weight matrix $A$ in the is mostly
composed of zeroes.
 \item The bias inside the convolution layer are not useful in this case, they are
substracted by the normalization and then added back by the Batch normalization
layer.
\end{enumerate}
\subsubsection*{2}
\begin{enumerate}[label=(\alph*)]
\item True
\item True
\item True
\item True
\end{enumerate}
\subsubsection*{3}

    \begin{center}
    \begin{tabular}{| c | c | c |}
     \hline
     Layers & Output volume dimensions & Number of parameters \\ [0.5ex]
     \hline
     Input & $64 \times 64 \times 1$ & 0  \\
     \hline
     Conv4-5-Pvalid-S2 & $31 \times 31 \times 5$  & $(4 \times 4 \times 5) + 5 = 85$ \\
     \hline
     Pool3 & $8 \times 8 \times 5$ &  0 \\
     \hline
     Conv3-5-Pvalid-S1 & $6 \times 6 \times 5$ & $(3 \times 3 \times 5) + 5 = 50$ \\
     \hline
     Pool2 & $3 \times 3 \times 5$ &  0 \\ [1ex]
     \hline
     FC5 & $5 \times 1 \times 1$ & $  (5 \times 45) + 5 = 230$ \\ [1ex]
     \hline
    \end{tabular}
    \end{center}
\subsubsection*{4}
Let's first treat each filter as if applied to a single channel. Then, the size
of the receptive field is $\sum_{l =1 }^{5}(k_{l}-1) + 1 = \sum_{l =1 }^{5}(5-1) + 1$.
In our case, we are applying each filter to 8 intermediate channels.
Therefore, the total receptive field is of size $21 \times 21 \times 8$
}
\end{exercise}

% Q3
\begin{exercise}[10] (\textbf{Mixture Models meet Neural Networks})
\Exercise{
\label{ex:mixture_model}

Consider modelling some data $\{(\vx_n, y_n)\}_{n=1}^N$, $\vx_n \in \mathbb{R}^d$, $y_n \in \{0,1\}$, using a mixture of logistic regression models, where we model each binary label $y_n$ by first picking one of the $K$ logistic regression models, based on the value of a latent variable $z_n \sim \text{Categorical}(\pi_1, ..., \pi_K)$ and then generating $y_n$ \textit{conditioned} on $z_n$ as $y_n \sim \text{Bernoulli}[\sigma(\vw_{z_n}^T \vx_n)]$, where $\sigma(\cdot)$ is the sigmoid activation function.

Now consider the \textit{marginal} probability of the label $y_n = 1$, given $\vx_n$, i.e., $p(y_n = 1 | \vx_n)$, and show that this quantity can also be thought of as the output of a neural network. Clearly specify what is the input layer, the hidden layer(s), activations, the output layer, and the connection weights of this neural network.
}


\Answer{
% Write your answer here
  Let's calculat the marginal probability of the label $y_{n} = 1$:
\begin{align*}
  p(y_{n} =1\vert \bold{x}_{n} ) &= \sum_{l = 1}^{K} p(y_{n}\vert z_{n}=l, \bold{x}_{n})p(z_{n} = l)\\
                           &= \sum_{l = 1}^{K} \sigma(\bold{w}_{l}^{T}\bold{x}_{n})\pi_{l}\\
                           &= \sum_{l = 1}^{K} \frac{\pi_{l}}{1 + e^{- \bold{w}_{l}^{T}\bold{x}_{n}}} \\
\end{align*}We can build the neural network as follows:
The input layer is the vector $x$.  The  hidden layer is there to calculate the
category of the latent variable $z$:
\begin{equation*}
  z = \text{softmax}(A\bold{x}) \in \R^{K}
\end{equation*}The first layer $z$ activation function is a softmax, the connection weights are
in lines of the matrix $A$. The output layer is the following:
\begin{equation*}
  y = W^{T} z,
\end{equation*}where $W = [\pi_{1}, ... \pi_{K}]^{T} \in \R^{K}$. Therefore, the
output is:
\begin{equation*}
  y = \sum_{l =1}^{K} \pi_{l}\sigma(a_{l}\bold{x}),
\end{equation*}where $a_{l}$ is the $l$-th line of the matrix $A$. We see that
$y = p(y_{n} =1\vert \bold{x}_{n} ) $.
}
\end{exercise}

% Q4
\begin{exercise}[10] (\textbf{Cross Entropy})
\Exercise{
\label{ex:cross_entropy}

Cross-entropy loss function (a popular loss function) is given by:

$$ce(p, x) = - x \log(p) - (1 - x) \log(1 - p)$$

This derivation assumes that $x$ is binary, i.e. $x \in \{0, 1\}$. However, the same loss function is often also used with real-valued $x \in (0, 1)$.

\begin{enumerate}
\item Derive the cross-entropy loss function using the maximum likelihood principle for $x \in \{0, 1\}$.
\item Suggest a probabilistic interpretation of the cross-entropy loss function when $x \in \left(0, 1\right)$. (HINT: KL divergence between two distributions)
\end{enumerate}
}

\Answer{
% Write your answer here
\subsubsection*{1}
  The probability mass function of a random variables $x$ following a binomial distribution is
  $$p^{x_{i}}(1-p)^{1-x_{i}} $$ Let's consider a sampling of $N$ of iid
  observations. The log likelihood over the joint probability mass function is
  \begin{equation*}
  \ln (p^{x}(1-p)^{1-x})^{N}=  N (x \ln (p(x)) + (1-x)\ln (1-p(x)))
  \end{equation*} The goal of the maximum likelihood principle is to maximize
  the aforementioned equation by treating $p$ as a parameter. Because maximizing
  an expression is the same as minimizing the negative of the same expression
  and the factor $N$ does not affect the result, 
  we have that:
  \begin{equation*}
	\hat p =  \argmin_{p \in \Theta}  -x \ln (p(x)) - (1-x)\ln (1-p(x)) = \argmin_{p \in \Theta} ce(p,x).
  \end{equation*}

\subsubsection*{2}
We can show that the cross-entropy function is nearly the same as the KL divergence
between two distributions:
\begin{align*}
  D_{KL}(X || P) &= \sum_{x  \in \mathcal{X}} x \ln 1/p - \sum_{x \in \mathcal{X}} p \ln 1/p\\
                 &= -\sum_{x  \in \mathcal{X}} x \ln p + \sum_{x \in \mathcal{X}} p \ln p\\
                 &= ce(p,x) + H(p)\\
                 &= ce(p,x) + \text{constant}.
\end{align*}
Therefore, we can think of the cross-entropy loss function as a good
approximation of the KL divergence. Therefore, minimizing the cross-entropy loss
while keeping $p$ constant is the same as minimizing the KL divergence.
}
\end{exercise}

% Q5
\begin{exercise} [1-4-2] (\textbf{Optimization})
\Exercise{
\label{ex:Optimization}

\item Suppose we want to minimize \(f(x, y) = y^2 + (y - x)^2\).
\begin{enumerate}
    \item Find the true minimum.
    \item For each step size given below, compute:
    \begin{enumerate}
        \item The value of (x1, y1) using the full gradient descent (not stochastic) at the starting point (x0, y0) = (1, 1).
        \item The L2 distance of (x1, y1) from the true minimum.
        
        Please note that the (x1, y1) is (x, y) coordinate after 1 step of gradient descent.

        \begin{enumerate}
            \item Step size \(s = 0.01\).
            \item Step size \(s = 0.1\).
            \item Step size \(s = 10\).
        \end{enumerate}
    \end{enumerate}
    \item What are the implications of using different step sizes in gradient descent, and what strategies can be employed to effectively address these implications?
\end{enumerate}
}

\Answer{
% Write your answer here
\subsubsection*{1}
The true minimum of $f$ is $(0,0)$. We can see it by noticing how $f$ is a
sum of squares, and therefore has a minumum only when $(x,y) = (0,0)$.
\subsubsection*{2}
Let's calculate the gradient of $f$:
\begin{equation*}
  \nabla f (1,1)= (-2x, 4y) =  (-2, 4)
\end{equation*}
i.
\\
By using a step size of $s=0.01$, we have:
\begin{align*}
  (x_{1}, y_{1}) &= (x_{0}, y_{0}) - s \nabla f(x_{0}, y_{0})\\
  &= (1,1) - 0.01 (-2,4)\\
    &= (0.98, 0.96)
\end{align*}
Thefore, the $L_{2}$ distance from the true minimum is:
$\sqrt[2]{0.98^{2} + 0.96^{2}} \approx 1.37$
\\
ii.
\\
By using a step size of $s=0.01$, we have:
\begin{align*}
  (x_{1}, y_{1}) &= (x_{0}, y_{0}) - s \nabla f(x_{0}, y_{0})\\
  &= (1,1) - 0.1 (-2,4)\\
    &= (0.8, 0.6)
\end{align*}
Thefore, the $L_{2}$ distance from the true minimum is:
$\sqrt[2]{0.8^{2}+ 0.6^{2}} = 1$
\\
iii.
\\
By using a step size of $s=10$, we have:
\begin{align*}
  (x_{1}, y_{1}) &= (x_{0}, y_{0}) - s \nabla f(x_{0}, y_{0})\\
  &= (1,1) - 10 (-2,4)\\
    &= (-19, -39)
\end{align*}
Thefore, the $L_{2}$ distance from the true minimum is:
$\sqrt[2]{19^{2}+ 39^{2}} \approx 43.38$
\subsubsection*{3}
A big step size might make it impossible to reach a local minimum. At the same
time, a small step size might be inefficient, and take too much time before
reaching the minimum. To make learning fsat and accurate, we can use momentum to
acccelerate the descent and make it smoother. We can also use a learning rate
schedule to use a 'big' step size when the parameters are far from a solution
and reduce it as the number of iterations in the gradient descent goes up.
}
\end{exercise}

\end{document}
